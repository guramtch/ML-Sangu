import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Data loading and processing
data_text = """
words,links,capital_words,spam_word_count,is_spam
788,7,25,10,1
590,5,5,8,1
967,9,19,9,1
174,2,1,1,0
162,2,3,5,0
210,0,0,1,0
223,7,21,3,1
288,3,6,1,0
171,1,0,1,0
199,5,17,2,1
928,9,5,4,1
64,2,0,5,0
80,9,8,8,1
270,0,4,5,0
794,0,27,6,1
206,0,2,2,0
113,0,2,4,0
230,2,3,1,0
335,2,1,10,1
356,2,1,8,1
474,9,19,7,1
874,1,1,5,1
451,10,30,6,1
83,3,0,0,0
574,9,1,5,1
289,0,1,1,0
229,3,24,6,1
840,7,24,5,1
769,10,3,5,1
234,2,2,1,0
221,1,2,0,0
284,2,3,1,0
132,1,0,0,0
383,10,10,1,1
109,2,17,6,1
198,3,19,4,1
101,1,7,7,1
210,2,6,3,1
102,2,0,2,0
101,0,9,2,0
158,1,2,2,0
739,5,5,5,1
87,3,0,2,0
181,1,3,0,0
871,8,5,4,1
223,0,0,0,0
814,0,7,9,1
246,1,2,4,0
106,1,0,2,0
679,10,7,6,1
226,2,2,0,0
210,0,1,1,0
282,1,1,1,0
979,5,1,7,1
788,3,10,3,1
766,10,11,10,1
30,0,1,1,0
270,2,5,2,1
451,1,2,1,0
47,1,8,1,0
841,9,1,1,1
224,1,0,2,0
438,2,3,2,0
220,1,0,0,0
548,5,2,10,1
377,2,0,2,0
185,2,21,6,1
274,2,0,2,0
228,2,8,0,0
297,5,6,2,1
630,5,5,4,1
318,7,26,5,1
143,8,30,6,1
394,9,5,5,1
827,1,26,10,1
232,1,0,1,0
987,1,2,1,1
951,8,8,6,1
107,4,18,8,1
160,8,6,1,1
687,3,13,1,1
137,2,0,0,0
275,2,2,2,0
703,4,6,10,1
27,0,2,0,0
185,4,26,2,1
233,0,0,1,0
90,2,21,6,1
691,3,26,3,1
97,7,4,6,1
299,2,6,7,1
213,1,3,0,0
78,1,9,2,0
297,0,1,2,0
37,0,2,1,0
988,6,0,3,1
992,2,7,1,1
393,0,2,5,0
525,3,20,1,1
266,0,0,3,0
149,2,0,0,0
367,3,2,2,0
232,0,4,8,1
138,6,13,5,1
153,1,2,0,0
656,9,13,3,1
137,3,5,8,1
289,0,14,6,1
119,5,1,6,0
194,2,2,4,0
468,9,12,7,1
918,6,1,5,1
211,0,18,0,1
125,3,26,2,1
292,2,0,1,0
40,1,5,0,0
784,4,19,9,1
41,0,1,2,0
201,0,7,2,0
64,1,5,2,0
204,2,1,2,0
48,5,9,1,0
394,9,3,8,1
241,2,3,1,0
271,2,2,0,0
245,0,2,3,0
260,0,0,0,0
42,1,1,2,0
151,0,0,2,0
166,0,1,3,0
297,1,3,2,0
99,0,1,1,0
353,2,4,2,0
273,2,0,3,0
255,8,21,9,1
290,1,3,6,0
345,2,8,6,1
163,0,0,2,0
149,1,0,1,0
279,0,1,0,0
593,4,20,3,1
156,3,9,5,1
417,4,4,8,1
67,4,7,2,0
299,0,2,0,0
205,1,1,1,0
268,0,2,1,0
87,1,3,1,0
238,1,0,0,0
717,8,22,7,1
177,3,19,4,1
114,8,4,2,1
511,3,26,5,1
337,9,25,4,1
187,0,2,0,0
945,2,9,1,1
739,6,30,6,1
909,8,13,7,1
73,2,4,1,0
152,1,3,2,0
490,1,2,6,0
118,8,3,0,1
328,0,12,6,1
195,3,12,9,1
189,2,3,2,0
185,0,3,1,0
174,5,6,2,0
193,2,3,1,0
218,4,3,1,0
181,2,2,0,0
429,4,21,9,1
91,1,9,5,0
247,2,0,0,0
138,5,8,7,1
91,0,3,0,0
166,4,1,2,0
139,1,0,3,0
429,2,1,2,0
201,2,9,2,0
149,2,8,0,0
189,4,2,0,0
203,2,3,1,0
283,1,1,1,0
82,3,26,9,1
299,7,10,8,1
609,6,5,2,1
122,1,1,1,0
271,4,4,2,0
231,4,3,2,0
58,1,0,1,0
602,8,28,6,1
119,2,9,6,1
198,1,3,0,0
100,2,2,1,0
242,0,5,1,0
811,10,10,8,1
171,6,14,10,1
56,0,0,5,0
210,4,0,1,0
38,1,6,1,0
127,0,0,2,0
254,4,18,9,1
142,9,3,5,1
82,0,2,1,0
22,0,3,3,0
205,1,8,0,0
387,3,19,3,1
104,1,10,0,0
102,1,2,0,0
118,9,13,10,1
65,3,2,7,1
277,2,8,1,0
951,6,21,10,1
130,8,10,3,1
157,2,13,5,1
963,5,24,1,1
201,2,16,8,1
440,3,25,0,1
107,2,0,2,0
985,3,10,3,1
126,3,21,9,1
267,0,3,6,0
403,0,2,0,0
203,2,2,0,0
184,8,2,6,1
62,0,1,1,0
184,1,0,8,1
160,2,2,2,0
792,2,18,3,1
211,1,2,2,0
196,7,19,1,1
381,4,6,2,0
678,3,22,3,1
925,6,5,10,1
349,10,25,6,1
328,10,30,10,1
573,10,30,3,1
257,2,0,1,0
458,5,22,9,1
421,2,3,0,0
245,1,5,5,1
164,2,3,1,0
123,7,3,3,1
225,2,27,10,1
996,9,2,8,1
150,0,1,1,0
452,6,12,6,1
131,5,26,3,1
197,0,0,1,0
223,0,0,1,0
531,4,10,1,1
125,2,2,2,0
794,4,1,0,1
853,10,18,3,1
218,2,2,1,0
601,2,25,5,1
64,5,9,2,1
220,2,0,1,0
269,0,1,0,0
102,6,19,7,1
79,2,3,6,0
215,5,2,5,0
647,2,5,1,1
34,2,1,0,0
751,6,1,9,1
161,1,3,1,0
87,1,28,7,1
184,0,4,0,0
22,2,0,2,0
214,0,2,3,0
170,0,3,1,0
214,1,2,1,0
368,2,16,2,1
157,0,5,4,0
180,2,5,8,1
191,1,0,2,0
23,1,2,0,0
63,2,5,2,0
50,0,1,2,0
606,10,17,7,1
671,7,7,9,1
959,3,13,10,1
965,10,10,8,1
106,1,0,0,0
832,4,16,1,1
962,10,8,3,1
65,7,15,6,1
658,3,16,10,1
286,0,9,1,0
288,5,0,1,0
72,2,1,2,0
789,0,14,0,1
536,9,5,10,1
177,0,3,4,0
440,1,3,2,0
142,1,21,0,1
189,2,3,1,0
130,0,0,0,0
211,1,0,2,0
434,0,1,4,0
405,1,3,2,0
199,4,24,8,1
662,5,6,7,1
294,7,27,3,1
178,3,1,0,0
82,2,3,2,0
273,0,0,2,0
119,0,27,7,1
54,1,3,3,0
70,4,5,6,1
491,1,10,3,0
580,3,26,3,1
239,2,26,10,1
460,4,0,1,0
651,9,11,10,1
497,6,2,4,1
233,1,0,1,0
216,0,6,6,0
340,2,9,8,1
1000,0,30,6,1
501,2,9,8,1
935,9,17,10,1
58,1,3,1,0
295,1,2,5,0
274,0,19,6,1
217,0,0,1,0
106,5,3,1,0
278,0,1,4,0
138,7,30,3,1
300,2,1,4,0
553,1,6,3,1
829,1,6,6,1
251,2,2,2,0
188,2,1,2,0
438,8,26,8,1
223,0,0,2,0
373,4,1,3,0
125,5,28,5,1
39,2,3,2,0
409,10,25,4,1
461,1,2,0,0
710,9,14,0,1
116,2,24,1,1
341,4,8,2,0
162,4,1,4,0
237,0,7,2,0
875,8,9,0,1
20,1,3,0,0
304,1,7,0,0
917,2,17,3,1
122,4,20,8,1
618,6,20,10,1
760,2,24,3,1
551,0,21,8,1
155,0,7,0,0
146,1,2,0,0
258,2,1,4,1
608,4,29,2,1
280,1,0,1,0
897,9,23,10,1
364,10,24,8,1
269,2,3,2,0
372,9,21,5,1
169,2,0,2,0
273,0,1,2,0
889,2,24,10,1
165,1,1,1,0
123,1,3,1,0
961,2,23,6,1
299,1,2,2,0
434,8,20,9,1
250,3,7,0,0
182,1,0,0,0
946,7,16,10,1
369,0,3,2,0
882,0,0,9,1
165,1,2,0,0
57,1,2,4,0
139,6,3,7,1
85,0,3,1,0
334,5,5,9,1
818,4,7,5,1
152,2,1,2,0
193,2,2,1,0
404,1,2,6,0
958,7,26,7,1
870,3,20,3,1
218,1,3,3,0
228,1,2,2,0
639,2,2,5,1
224,0,3,2,0
281,2,4,6,1
163,6,23,3,1
243,1,3,2,0
195,5,6,1,1
108,10,24,10,1
155,2,17,5,1
76,2,2,2,0
129,10,1,5,1
193,10,7,4,1
281,1,2,1,0
184,4,1,2,0
122,5,3,3,1
112,2,6,0,0
33,0,1,1,0
116,2,5,0,0
109,2,10,0,1
283,0,2,0,0
759,8,11,8,1
62,2,10,1,0
910,9,9,5,1
580,9,26,4,1
103,2,25,7,1
215,2,0,4,0
146,0,2,1,0
582,9,9,3,1
52,10,2,2,1
737,2,22,1,1
195,0,7,1,0
134,4,29,7,1
123,9,2,6,1
278,4,0,0,0
123,1,2,0,0
170,5,1,0,0
637,4,7,9,1
875,2,21,7,1
738,7,22,8,1
487,2,0,0,0
981,3,13,7,1
481,7,6,4,1
694,1,29,5,1
888,2,8,5,1
882,1,18,9,1
140,10,18,9,1
470,1,3,1,0
237,6,0,5,1
151,0,4,2,0
181,8,6,6,1
160,3,2,9,1
40,4,3,0,0
694,3,23,5,1
73,4,3,0,0
54,2,3,2,0
951,3,16,1,1
165,0,20,4,1
264,1,2,2,0
160,1,1,0,0
289,2,3,0,0
89,1,2,1,0
188,9,12,6,1
383,0,19,4,1
151,3,11,4,1
129,2,1,1,0
71,5,2,1,0
147,4,6,3,1
286,3,23,3,1
474,7,27,5,1
350,6,3,5,1
263,2,3,0,0
169,2,3,3,0
73,1,3,1,0
182,2,15,8,1
37,1,3,2,0
135,5,3,0,0
442,4,2,10,1
45,1,1,1,0
157,2,0,0,0
954,9,8,6,1
645,6,23,3,1
92,6,11,10,1
110,9,12,0,1
104,3,3,8,1
147,4,2,4,1
234,4,25,5,1
266,1,1,0,0
144,5,29,6,1
368,1,13,4,1
298,2,1,6,0
436,10,10,1,1
187,2,0,2,0
26,1,2,3,0
228,2,8,6,0
112,3,12,3,1
599,2,18,9,1
60,1,0,0,0
871,3,10,8,1
219,0,0,0,0
211,10,8,5,1
207,4,3,0,0
868,10,16,5,1
794,2,11,9,1
244,3,1,2,0
610,1,8,7,1
64,1,4,2,0
313,1,2,1,0
715,2,14,7,1
242,1,1,2,0
151,1,0,1,0
324,0,8,3,0
396,4,16,4,1
280,2,3,2,0
626,7,4,4,1
307,4,26,10,1
892,5,6,5,1
665,2,23,0,1
242,2,3,0,0
749,1,4,3,1
174,1,0,4,1
582,8,24,5,1
333,4,3,3,1
97,0,3,0,0
206,0,0,0,0
31,0,3,1,0
196,0,1,2,0
235,1,0,1,0
937,8,7,8,1
121,8,10,8,1
286,3,2,2,0
196,10,1,8,1
259,2,0,2,0
731,3,13,7,1
184,2,10,2,0
130,1,3,2,0
224,5,7,0,0
483,6,21,10,1
293,6,27,10,1
124,0,28,7,1
75,0,2,1,0
82,7,28,9,1
223,0,6,0,0
130,2,9,3,1
185,5,3,2,0
31,1,2,1,0
154,2,23,9,1
514,5,7,10,1
269,0,15,5,1
99,2,16,9,1
150,2,3,1,0
96,0,3,1,0
934,7,11,4,1
804,10,15,3,1
728,2,22,3,1
186,10,28,6,1
186,2,5,6,1
177,10,6,3,1
175,2,2,2,0
151,9,23,6,1
130,3,13,5,1
343,5,5,0,0
135,3,23,9,1
91,3,0,5,0
258,2,2,4,0
364,10,11,1,1
31,0,2,5,0
379,2,11,8,1
894,4,11,2,1
216,2,3,2,0
242,3,1,6,0
174,1,3,2,0
125,5,8,3,1
80,2,0,2,0
371,4,7,6,1
239,1,1,2,0
165,0,10,1,0
740,9,17,3,1
233,5,1,2,0
838,2,2,3,1
174,7,12,10,1
468,1,4,1,0
689,10,21,1,1
994,9,20,1,1
170,2,2,2,0
936,3,29,4,1
84,2,0,3,1
270,1,3,4,0
51,1,2,0,0
324,5,25,8,1
216,1,1,2,0
792,6,28,7,1
540,7,28,10,1
198,4,27,10,1
899,2,21,7,1
484,2,1,3,0
186,0,2,1,0
99,0,0,2,0
225,4,10,2,0
261,2,0,2,0
292,2,3,0,0
574,7,0,10,1
498,10,17,7,1
176,1,0,0,0
527,4,20,9,1
88,9,27,6,1
925,3,23,2,1
210,3,2,2,0
29,4,2,0,0
615,2,14,4,1
352,5,23,4,1
253,0,3,1,0
239,0,1,2,0
87,0,0,1,0
819,9,15,4,1
134,5,1,0,0
872,10,17,8,1
119,0,3,1,0
652,2,9,6,1
420,0,11,4,1
339,5,19,9,1
850,6,22,10,1
394,8,1,3,1
186,6,7,7,1
174,0,3,6,0
272,0,3,0,0
380,1,0,1,0
111,0,3,3,0
197,0,1,6,0
33,1,2,1,0
533,2,5,4,1
745,10,18,6,1
119,2,10,1,0
272,1,0,0,0
254,8,5,5,1
156,4,4,0,1
647,2,11,1,1
679,8,7,10,1
99,2,21,7,1
250,10,29,5,1
173,9,11,1,1
286,2,1,2,0
140,1,3,4,0
161,6,22,3,1
103,10,5,7,1
288,4,0,4,0
254,0,1,2,0
187,2,3,0,0
359,0,2,2,0
560,5,17,3,1
832,4,25,7,1
160,2,2,0,0
168,3,27,5,1
306,1,6,0,0
187,4,18,3,1
71,1,24,9,1
983,5,4,7,1
174,0,4,2,0
125,2,3,6,0
810,4,15,5,1
875,3,30,8,1
301,10,30,3,1
602,10,16,6,1
294,0,2,1,0
174,4,7,2,0
155,4,15,10,1
492,10,5,3,1
142,9,8,8,1
25,4,0,5,0
147,2,21,5,1
140,0,2,1,0
200,0,1,1,0
187,2,4,3,0
145,1,3,2,0
656,7,29,5,1
141,10,29,5,1
163,2,0,0,0
139,0,3,1,0
282,0,5,6,0
65,1,5,2,0
990,3,6,5,1
132,0,10,0,0
112,1,2,0,0
198,0,2,2,0
28,1,2,2,0
127,0,3,6,0
141,2,2,1,0
250,5,5,2,0
750,2,13,6,1
37,2,2,2,0
596,2,6,2,1
398,1,0,1,0
786,4,6,5,1
92,0,3,0,0
50,0,3,4,1
118,1,3,0,0
1000,3,23,3,1
35,5,3,1,0
161,1,3,1,0
127,2,26,7,1
81,1,8,0,0
344,9,1,1,1
180,3,2,0,0
121,0,0,2,0
171,4,1,6,1
192,3,16,3,1
249,0,3,2,0
50,2,6,8,1
473,1,0,2,0
242,2,2,2,0
181,0,2,2,0
300,1,4,1,0
243,4,14,3,1
705,9,23,9,1
418,0,2,2,0
64,0,2,0,0
869,7,12,0,1
108,1,4,0,0
859,1,28,5,1
45,2,3,4,0
134,2,3,2,0
69,3,29,3,1
38,2,0,6,0
133,0,2,0,0
713,5,5,8,1
201,2,1,2,0
281,1,2,0,0
126,4,3,1,0
125,0,0,0,0
76,3,5,9,1
203,0,10,2,0
961,7,1,4,1
170,8,23,4,1
74,1,3,0,0
296,2,3,1,0
73,2,2,0,0
121,0,17,4,1
622,5,1,7,1
409,0,3,3,0
285,2,3,1,0
140,2,0,1,0
157,3,15,3,1
438,3,1,3,1
558,6,24,5,1
180,9,3,10,1
192,10,23,6,1
172,0,1,2,0
282,1,0,0,0
44,0,0,2,0
485,1,0,2,0
209,4,2,4,1
48,1,0,0,0
549,1,18,7,1
157,3,13,9,1
204,1,1,0,0
239,1,3,6,0
184,2,4,1,0
150,0,10,0,0
283,0,2,2,0
260,2,7,1,0
848,8,0,3,1
36,2,2,6,0
58,0,2,2,0
291,5,1,0,0
292,0,4,0,0
895,2,23,2,1
369,2,4,6,0
97,2,2,2,0
302,6,21,8,1
106,0,1,3,0
631,6,22,0,1
133,1,2,2,0
675,9,26,2,1
355,5,13,4,1
137,0,3,1,0
846,3,22,7,1
94,1,3,0,0
959,5,21,5,1
237,3,0,6,0
263,1,0,2,0
336,3,27,0,1
682,3,13,10,1
27,2,2,0,0
489,1,4,5,1
363,2,0,0,0
121,3,3,5,1
224,1,3,0,0
131,2,27,10,1
291,3,0,6,0
245,1,3,1,0
79,0,8,6,0
305,8,12,4,1
117,2,3,4,0
313,10,28,10,1
178,0,1,0,0
810,7,8,9,1
193,4,1,2,0
269,4,1,2,0
426,1,3,1,0
429,9,27,9,1
290,0,1,1,0
156,1,0,2,0
824,2,4,3,1
23,4,2,6,0
123,3,2,2,0
208,3,19,8,1
715,6,26,5,1
223,2,1,2,0
167,2,17,6,1
144,2,3,1,0
217,8,24,2,1
117,2,29,9,1
680,4,8,0,1
493,2,5,1,0
730,3,17,10,1
128,0,0,0,0
162,8,12,8,1
182,2,9,10,1
723,6,18,3,1
278,2,7,7,1
596,1,3,5,1
967,3,28,3,1
59,1,0,1,0
54,1,1,6,0
322,5,2,1,0
495,10,30,10,1
183,3,4,3,1
99,1,0,2,0
101,9,16,3,1
188,2,9,3,0
137,2,30,10,1
660,6,4,7,1
26,3,2,5,0
768,5,18,3,1
496,4,4,0,1
531,3,17,7,1
290,2,2,2,0
294,4,15,9,1
60,3,1,9,1
106,0,2,1,0
133,0,10,2,0
139,5,0,1,0
191,2,0,2,0
152,5,0,1,0
93,1,0,2,0
137,2,2,2,0
159,3,0,2,0
416,2,0,4,0
163,0,3,4,0
136,9,10,3,1
182,2,0,5,0
357,9,16,1,1
89,2,2,2,0
"""

from io import StringIO
data = pd.read_csv(StringIO(data_text))

# Split features and target
X = data[['words', 'links', 'capital_words', 'spam_word_count']]
y = data['is_spam']

# Split in 70% train, 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model: Logistic Regression
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Coefficients
coefficients = dict(zip(X.columns, model.coef_[0]))
intercept = model.intercept_[0]

# Test model
y_pred = model.predict(X_test)
conf_mat = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

# Function to classify new email text (simulate feature extraction)
def classify_email(words, links, capital_words, spam_word_count):
    features = np.array([[words, links, capital_words, spam_word_count]])
    pred = model.predict(features)[0]
    prob = model.predict_proba(features)[0][1]
    return pred, prob

# Compose spam email text simulation for classification:
example_email = {
    'words': 900,
    'links': 8,
    'capital_words': 25,
    'spam_word_count': 9
}
predicted_class, probability = classify_email(**example_email)

# Visualizations
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(data=data, x='spam_word_count', hue='is_spam', multiple='stack', bins=20)
plt.title('Distribution of Spam Word Count by Class')
plt.xlabel('Spam Word Count')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
sns.scatterplot(data=data, x='words', y='capital_words', hue='is_spam', alpha=0.6)
plt.title('Words vs Capital Words Colored by Spam Class')
plt.xlabel('Number of Words')
plt.ylabel('Capital Words')

plt.tight_layout()
plt.show()

# Output report
print("Logistic Regression Coefficients:")
for feature, coef in coefficients.items():
    print(f"  {feature}: {coef:.4f}")
print(f"Intercept: {intercept:.4f}\n")

print("Confusion Matrix (Test Data):")
print(conf_mat)
print(f"\nAccuracy on test data: {accuracy:.4f}\n")

print("Example email features for classification:")
print(example_email)
print(f"Predicted as: {'Spam' if predicted_class==1 else 'Not Spam'} with probability {probability:.4f}")
